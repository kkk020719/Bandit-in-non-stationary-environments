# Bandit-in-non-stationary-environments
In this project we compare the performance of various multi-arm bandit algorithms in different non-stationary settings. Specifically, we evaluate empirically the performance of the EXP3, Ïµ-greedy, and Thompson-Sampling algorithms, as well as the discounted and standard variations of UCB. We consider non-stationary environments in the 3-arm bandit scenario whereby the reward distributions of each arm undergo stationary phases interrupted by sudden distribution-shifts. Through experiments, we discuss which forms of non-stationarity are favoured by the aforementioned algorithms.

The notebook includes all the required classes, setup and functions to plot the final graph.
